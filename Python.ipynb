{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/NVK4YI0QUXQEzwvrVtsq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdulHakkam/SDGP/blob/ML-Model/Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xct0D5wB-kO5",
        "outputId": "e6210690-2bf2-460a-cb7c-ee88390e6b7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot) (5.5.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (1.21.5)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot) (0.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot) (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.3.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->livelossplot) (0.7.0)\n",
            "tfds.core.DatasetInfo(\n",
            "    name='stanford_dogs',\n",
            "    version=0.2.0,\n",
            "    description='The Stanford Dogs dataset contains images of 120 breeds of dogs from around\n",
            "the world. This dataset has been built using images and annotation from\n",
            "ImageNet for the task of fine-grained image categorization. There are\n",
            "20,580 images, out of which 12,000 are used for training and 8580 for\n",
            "testing. Class labels and bounding box annotations are provided\n",
            "for all the 12,000 images.',\n",
            "    homepage='http://vision.stanford.edu/aditya86/ImageNetDogs/main.html',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'image/filename': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=120),\n",
            "        'objects': Sequence({\n",
            "            'bbox': BBoxFeature(shape=(4,), dtype=tf.float32),\n",
            "        }),\n",
            "    }),\n",
            "    total_num_examples=20580,\n",
            "    splits={\n",
            "        'test': 8580,\n",
            "        'train': 12000,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"@inproceedings{KhoslaYaoJayadevaprakashFeiFei_FGVC2011,\n",
            "    author = \"Aditya Khosla and Nityananda Jayadevaprakash and Bangpeng Yao and\n",
            "              Li Fei-Fei\",\n",
            "    title = \"Novel Dataset for Fine-Grained Image Categorization\",\n",
            "    booktitle = \"First Workshop on Fine-Grained Visual Categorization,\n",
            "                 IEEE Conference on Computer Vision and Pattern Recognition\",\n",
            "    year = \"2011\",\n",
            "    month = \"June\",\n",
            "    address = \"Colorado Springs, CO\",\n",
            "    }\n",
            "    @inproceedings{imagenet_cvpr09,\n",
            "            AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and\n",
            "                      Li, K. and Fei-Fei, L.},\n",
            "            TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},\n",
            "            BOOKTITLE = {CVPR09},\n",
            "            YEAR = {2009},\n",
            "            BIBSOURCE = \"http://www.image-net.org/papers/imagenet_cvpr09.bib\"}\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import pandas as pd\n",
        "import sklearn.metrics as metrics\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "!pip install livelossplot\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "(ds_train,ds_test),info = tfds.load('StanfordDogs', split=[\"train\",\"test\"], with_info=True,as_supervised=True)  #retreiving dataset\n",
        "print(info)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(image,label):                                                   #preprocess dataset before training by converting type and resizing it to 224,224,3\n",
        "\n",
        "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    image = tf.image.resize(image, (244,244), method='nearest')\n",
        "  \n",
        "    label = tf.one_hot(label,120)\n",
        "\n",
        "    return image, label\n",
        "def prepare(dataset, batch_size=None):                                         #prepares the dataset for training by preprocessing,shuffling and then prefetching the data                       \n",
        "    ds = dataset.map(preprocess, num_parallel_calls=4)\n",
        "    ds = ds.shuffle(buffer_size=1000)\n",
        "    if batch_size:\n",
        "      ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "ds_train = prepare(ds_train,batch_size=32)\n",
        "ds_test =prepare(ds_test,batch_size=32)\n",
        "\n",
        "\n",
        "\n",
        "IMG_SHAPE=(244,244,3)\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               \n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')\n",
        "base_model.trainable = False\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(120, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "nvr9rMr-G50J",
        "outputId": "eb509996-e7f7-40f5-f6be-56a40dfece3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "k8jF0ubNGGmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_using_ops(images, labels):\n",
        "\t\n",
        "\timages = tf.image.random_flip_left_right(images)\n",
        "\timages = tf.image.random_flip_up_down(images)\n",
        "\timages = tf.image.rot90(images)\n",
        "\t\n",
        "\treturn (images, labels)\n",
        " \n",
        " \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gr4BJDOhGHKM",
        "outputId": "cd46ed8c-f49a-445b-a3e8-4c562f04c1aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4aa0b3b8a601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     horizontal_flip=True)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute 'len'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adamax(0.0001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy',tf.keras.metrics.TrueNegatives(),tf.keras.metrics.FalseNegatives(),tf.keras.metrics.TruePositives()]\n",
        "              )\n",
        "\n",
        "history = model.fit(ds_train,epochs=30,validation_data=ds_test,callbacks=[PlotLossesKeras()])"
      ],
      "metadata": {
        "id": "ikdJWom7yIqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}